{'augment': True,
 'backbone': {'kwargs': {'embed_dim': 768,
                         'n_tasks': 10,
                         'num_heads': 12,
                         'pretrained': True,
                         'rank': 5},
              'name': 'vit_inflora'},
 'batch_size': 128,
 'buffer': {'kwargs': {'batch_size': 128, 'buffer_size': 0, 'strategy': 'None'},
            'name': 'LinearBuffer'},
 'classifier': {'kwargs': {'EPSILON': 1e-08,
                           'dataset': 'cifar100',
                           'epoch': 20,
                           'fc_lrate': 0.01,
                           'feat_dim': 768,
                           'lamb': 0.95,
                           'lame': 1.0,
                           'lrate': 0.001,
                           'lrate_decay': 0.1,
                           'num_class': 100,
                           'task_num': 10,
                           'weight_decay': 0.001},
                'name': 'InfLoRA_CA1'},
 'data_root': '/data1/student/zzq/LibContinual/data/cifar100',
 'dataset': 'cifar',
 'deterministic': True,
 'device_ids': 5,
 'epoch': 20,
 'image_size': 32,
 'inc_cls_num': 10,
 'includes': ['headers/data.yaml', 'headers/device.yaml', 'headers/model.yaml'],
 'init_cls_num': 10,
 'init_epoch': 20,
 'lr_scheduler': {'kwargs': {'gamma': 1.0, 'step_size': 20}, 'name': 'StepLR'},
 'n_gpu': 1,
 'optim': 'sgd',
 'optimizer': {'kwargs': {'lr': 0.1}, 'name': 'SGD'},
 'pin_memory': False,
 'rank': 7,
 'save_path': './inflora_ca',
 'seed': 0,
 'task_num': 10,
 'val_per_epoch': 10,
 'warmup': 0,
 'workers': 16}
ViTZoo(
  (feat): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): Sequential(
      (0): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (1): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (2): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (3): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (4): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (5): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (6): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (7): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (8): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (9): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (10): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (11): Block_LoRA(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention_LoRA(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (lora_A_k): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_k): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
          (lora_A_v): ModuleList(
            (0-9): 10 x Linear(in_features=768, out_features=5, bias=False)
          )
          (lora_B_v): ModuleList(
            (0-9): 10 x Linear(in_features=5, out_features=768, bias=False)
          )
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
Trainable params in the model: 87795656
================Task 0 Start!================
Parameters to be updated: {'backbone.feat.blocks.7.attn.lora_B_v.0.weight', 'backbone.feat.blocks.4.attn.lora_B_v.0.weight', 'backbone.feat.blocks.5.attn.lora_B_v.0.weight', 'backbone.feat.blocks.2.attn.lora_B_v.0.weight', 'backbone.feat.blocks.6.attn.lora_B_v.0.weight', 'backbone.feat.blocks.0.attn.lora_B_k.0.weight', 'backbone.feat.blocks.6.attn.lora_B_k.0.weight', 'backbone.feat.blocks.8.attn.lora_B_k.0.weight', 'backbone.feat.blocks.1.attn.lora_B_k.0.weight', 'backbone.feat.blocks.1.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'backbone.feat.blocks.10.attn.lora_B_v.0.weight', 'backbone.feat.blocks.10.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'backbone.feat.blocks.7.attn.lora_B_k.0.weight', 'backbone.feat.blocks.4.attn.lora_B_k.0.weight', 'backbone.feat.blocks.3.attn.lora_B_v.0.weight', 'backbone.feat.blocks.0.attn.lora_B_v.0.weight', 'backbone.feat.blocks.11.attn.lora_B_k.0.weight', 'backbone.feat.blocks.3.attn.lora_B_k.0.weight', 'backbone.feat.blocks.9.attn.lora_B_v.0.weight', 'backbone.feat.blocks.8.attn.lora_B_v.0.weight', 'backbone.feat.blocks.11.attn.lora_B_v.0.weight', 'backbone.feat.blocks.9.attn.lora_B_k.0.weight', 'backbone.feat.blocks.2.attn.lora_B_k.0.weight', 'backbone.feat.blocks.5.attn.lora_B_k.0.weight'}
================Task 0 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.7352 	Average Acc: 78.18 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1625 	Average Acc: 94.98 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1405 	Average Acc: 95.53 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1290 	Average Acc: 95.82 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1093 	Average Acc: 96.54 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.0960 	Average Acc: 96.99 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.0977 	Average Acc: 96.78 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.0944 	Average Acc: 97.17 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.0966 	Average Acc: 97.11 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.0880 	Average Acc: 97.23 
================ Test on the test set ================
 * Average Acc: 99.41 Best acc 99.41
 * Per-Task Acc:[99.41]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.0954 	Average Acc: 96.95 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.0938 	Average Acc: 97.13 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1013 	Average Acc: 96.62 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.0962 	Average Acc: 97.09 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.0772 	Average Acc: 97.44 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.0889 	Average Acc: 97.03 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.0955 	Average Acc: 96.80 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1022 	Average Acc: 96.70 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.0953 	Average Acc: 96.93 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.0739 	Average Acc: 97.64 
================ Test on the test set ================
 * Average Acc: 99.59 Best acc 99.59
 * Per-Task Acc:[99.59]
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768
Layer 2 : 9/768
Layer 3 : 9/768
Layer 4 : 9/768
Layer 5 : 10/768
Layer 6 : 13/768
Layer 7 : 14/768
Layer 8 : 23/768
Layer 9 : 39/768
Layer 10 : 36/768
Layer 11 : 11/768
Layer 12 : 14/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 0 Testing!================
 * Average Acc: 99.61 Best acc 99.61
 * Per-Task Acc:[99.61]
这是我个人设置的准确率记录
ACC_1:  [99.61]
平均ACC: 99.61
================Task 1 Start!================
Parameters to be updated: {'backbone.feat.blocks.6.attn.lora_B_k.1.weight', 'backbone.feat.blocks.11.attn.lora_B_v.1.weight', 'backbone.feat.blocks.3.attn.lora_B_k.1.weight', 'backbone.feat.blocks.10.attn.lora_B_v.1.weight', 'backbone.feat.blocks.1.attn.lora_B_v.1.weight', 'backbone.feat.blocks.7.attn.lora_B_k.1.weight', 'backbone.feat.blocks.6.attn.lora_B_v.1.weight', 'backbone.feat.blocks.9.attn.lora_B_v.1.weight', 'backbone.feat.blocks.0.attn.lora_B_k.1.weight', 'backbone.feat.blocks.2.attn.lora_B_v.1.weight', 'backbone.feat.blocks.4.attn.lora_B_v.1.weight', 'backbone.feat.blocks.5.attn.lora_B_k.1.weight', 'backbone.feat.blocks.7.attn.lora_B_v.1.weight', 'backbone.feat.blocks.3.attn.lora_B_v.1.weight', 'backbone.feat.blocks.5.attn.lora_B_v.1.weight', 'backbone.feat.blocks.10.attn.lora_B_k.1.weight', 'classifier_pool.1.bias', 'backbone.feat.blocks.0.attn.lora_B_v.1.weight', 'backbone.feat.blocks.11.attn.lora_B_k.1.weight', 'classifier_pool.1.weight', 'backbone.feat.blocks.4.attn.lora_B_k.1.weight', 'backbone.feat.blocks.1.attn.lora_B_k.1.weight', 'backbone.feat.blocks.8.attn.lora_B_k.1.weight', 'backbone.feat.blocks.9.attn.lora_B_k.1.weight', 'backbone.feat.blocks.2.attn.lora_B_k.1.weight', 'backbone.feat.blocks.8.attn.lora_B_v.1.weight'}
================Task 1 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.5209 	Average Acc: 84.55 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1832 	Average Acc: 94.14 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1812 	Average Acc: 94.08 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1720 	Average Acc: 94.06 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1631 	Average Acc: 94.57 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1439 	Average Acc: 95.35 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1648 	Average Acc: 95.20 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1408 	Average Acc: 95.31 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1360 	Average Acc: 95.78 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1407 	Average Acc: 95.41 
================ Test on the test set ================
 * Average Acc: 97.19 Best acc 97.19
 * Per-Task Acc:[99.02, 95.35]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1270 	Average Acc: 95.92 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1356 	Average Acc: 95.61 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1364 	Average Acc: 95.45 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1297 	Average Acc: 95.55 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1231 	Average Acc: 95.94 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1373 	Average Acc: 95.25 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1280 	Average Acc: 95.68 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1377 	Average Acc: 95.53 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1260 	Average Acc: 95.80 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1310 	Average Acc: 95.39 
================ Test on the test set ================
 * Average Acc: 97.51 Best acc 97.51
 * Per-Task Acc:[99.1, 95.93]
Threshold:  0.955
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768
Layer 2 : 10/768
Layer 3 : 11/768
Layer 4 : 11/768
Layer 5 : 14/768
Layer 6 : 17/768
Layer 7 : 20/768
Layer 8 : 31/768
Layer 9 : 61/768
Layer 10 : 67/768
Layer 11 : 23/768
Layer 12 : 39/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 1 Testing!================
 * Average Acc: 97.85 Best acc 97.85
 * Per-Task Acc:[98.93, 96.78]
这是我个人设置的准确率记录
ACC_2:  [99.61, 97.85]
平均ACC: 98.72999999999999
================Task 2 Start!================
Parameters to be updated: {'backbone.feat.blocks.7.attn.lora_B_v.2.weight', 'backbone.feat.blocks.10.attn.lora_B_v.2.weight', 'backbone.feat.blocks.11.attn.lora_B_v.2.weight', 'backbone.feat.blocks.11.attn.lora_B_k.2.weight', 'backbone.feat.blocks.5.attn.lora_B_k.2.weight', 'backbone.feat.blocks.4.attn.lora_B_v.2.weight', 'backbone.feat.blocks.8.attn.lora_B_k.2.weight', 'backbone.feat.blocks.1.attn.lora_B_k.2.weight', 'backbone.feat.blocks.10.attn.lora_B_k.2.weight', 'backbone.feat.blocks.8.attn.lora_B_v.2.weight', 'backbone.feat.blocks.4.attn.lora_B_k.2.weight', 'backbone.feat.blocks.6.attn.lora_B_v.2.weight', 'backbone.feat.blocks.3.attn.lora_B_k.2.weight', 'backbone.feat.blocks.1.attn.lora_B_v.2.weight', 'backbone.feat.blocks.9.attn.lora_B_k.2.weight', 'backbone.feat.blocks.0.attn.lora_B_k.2.weight', 'backbone.feat.blocks.2.attn.lora_B_k.2.weight', 'backbone.feat.blocks.3.attn.lora_B_v.2.weight', 'backbone.feat.blocks.5.attn.lora_B_v.2.weight', 'backbone.feat.blocks.0.attn.lora_B_v.2.weight', 'backbone.feat.blocks.9.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'backbone.feat.blocks.2.attn.lora_B_v.2.weight', 'backbone.feat.blocks.6.attn.lora_B_k.2.weight', 'backbone.feat.blocks.7.attn.lora_B_k.2.weight', 'classifier_pool.2.weight'}
================Task 2 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.5481 	Average Acc: 83.38 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1908 	Average Acc: 93.85 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1801 	Average Acc: 94.32 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1616 	Average Acc: 94.28 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1561 	Average Acc: 94.22 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1601 	Average Acc: 94.53 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1591 	Average Acc: 94.61 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1494 	Average Acc: 94.92 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1393 	Average Acc: 95.53 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1474 	Average Acc: 95.18 
================ Test on the test set ================
 * Average Acc: 96.43 Best acc 96.43
 * Per-Task Acc:[97.78, 95.1, 96.39]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1287 	Average Acc: 95.76 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1418 	Average Acc: 95.27 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1471 	Average Acc: 95.49 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1471 	Average Acc: 95.10 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1309 	Average Acc: 95.74 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1379 	Average Acc: 95.39 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1411 	Average Acc: 95.14 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1350 	Average Acc: 95.47 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1450 	Average Acc: 95.74 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1407 	Average Acc: 95.14 
================ Test on the test set ================
 * Average Acc: 96.40 Best acc 96.43
 * Per-Task Acc:[97.9, 94.88, 96.42]
Threshold:  0.96
Skip Updating GPM for layer: 1
Skip Updating GPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768
Layer 2 : 10/768
Layer 3 : 12/768
Layer 4 : 12/768
Layer 5 : 15/768
Layer 6 : 19/768
Layer 7 : 23/768
Layer 8 : 36/768
Layer 9 : 74/768
Layer 10 : 82/768
Layer 11 : 29/768
Layer 12 : 52/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 2 Testing!================
 * Average Acc: 96.97 Best acc 96.97
 * Per-Task Acc:[98.24, 95.91, 96.75]
这是我个人设置的准确率记录
ACC_3:  [99.61, 97.85, 96.97]
平均ACC: 98.14333333333332
================Task 3 Start!================
Parameters to be updated: {'backbone.feat.blocks.5.attn.lora_B_k.3.weight', 'backbone.feat.blocks.0.attn.lora_B_k.3.weight', 'backbone.feat.blocks.8.attn.lora_B_k.3.weight', 'backbone.feat.blocks.9.attn.lora_B_v.3.weight', 'backbone.feat.blocks.4.attn.lora_B_v.3.weight', 'backbone.feat.blocks.8.attn.lora_B_v.3.weight', 'backbone.feat.blocks.10.attn.lora_B_v.3.weight', 'backbone.feat.blocks.1.attn.lora_B_k.3.weight', 'backbone.feat.blocks.10.attn.lora_B_k.3.weight', 'backbone.feat.blocks.3.attn.lora_B_k.3.weight', 'backbone.feat.blocks.6.attn.lora_B_v.3.weight', 'classifier_pool.3.weight', 'backbone.feat.blocks.1.attn.lora_B_v.3.weight', 'backbone.feat.blocks.4.attn.lora_B_k.3.weight', 'backbone.feat.blocks.0.attn.lora_B_v.3.weight', 'backbone.feat.blocks.2.attn.lora_B_k.3.weight', 'backbone.feat.blocks.11.attn.lora_B_v.3.weight', 'backbone.feat.blocks.3.attn.lora_B_v.3.weight', 'backbone.feat.blocks.2.attn.lora_B_v.3.weight', 'backbone.feat.blocks.11.attn.lora_B_k.3.weight', 'backbone.feat.blocks.7.attn.lora_B_v.3.weight', 'classifier_pool.3.bias', 'backbone.feat.blocks.5.attn.lora_B_v.3.weight', 'backbone.feat.blocks.6.attn.lora_B_k.3.weight', 'backbone.feat.blocks.7.attn.lora_B_k.3.weight', 'backbone.feat.blocks.9.attn.lora_B_k.3.weight'}
================Task 3 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.4720 	Average Acc: 85.92 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1411 	Average Acc: 95.47 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1140 	Average Acc: 96.41 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1228 	Average Acc: 95.96 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1003 	Average Acc: 96.70 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1245 	Average Acc: 96.09 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1034 	Average Acc: 96.66 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1016 	Average Acc: 96.76 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.0900 	Average Acc: 97.38 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1031 	Average Acc: 96.70 
================ Test on the test set ================
 * Average Acc: 95.43 Best acc 95.43
 * Per-Task Acc:[96.88, 94.69, 96.44, 93.7]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.0936 	Average Acc: 97.01 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.0870 	Average Acc: 97.46 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.0909 	Average Acc: 97.05 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.0885 	Average Acc: 97.05 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.0845 	Average Acc: 97.17 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.0854 	Average Acc: 97.44 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.0920 	Average Acc: 97.38 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.0876 	Average Acc: 97.03 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1160 	Average Acc: 96.60 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.0831 	Average Acc: 97.42 
================ Test on the test set ================
 * Average Acc: 95.49 Best acc 95.49
 * Per-Task Acc:[96.35, 94.08, 96.17, 95.35]
Threshold:  0.965
Skip Updating GPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768
Layer 2 : 11/768
Layer 3 : 13/768
Layer 4 : 13/768
Layer 5 : 18/768
Layer 6 : 22/768
Layer 7 : 27/768
Layer 8 : 45/768
Layer 9 : 90/768
Layer 10 : 99/768
Layer 11 : 40/768
Layer 12 : 69/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 3 Testing!================
 * Average Acc: 95.83 Best acc 95.83
 * Per-Task Acc:[96.03, 94.71, 96.42, 96.15]
这是我个人设置的准确率记录
ACC_4:  [99.61, 97.85, 96.97, 95.83]
平均ACC: 97.56499999999998
================Task 4 Start!================
Parameters to be updated: {'backbone.feat.blocks.1.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'backbone.feat.blocks.4.attn.lora_B_k.4.weight', 'backbone.feat.blocks.9.attn.lora_B_v.4.weight', 'backbone.feat.blocks.11.attn.lora_B_k.4.weight', 'backbone.feat.blocks.3.attn.lora_B_v.4.weight', 'backbone.feat.blocks.2.attn.lora_B_v.4.weight', 'backbone.feat.blocks.0.attn.lora_B_v.4.weight', 'backbone.feat.blocks.9.attn.lora_B_k.4.weight', 'backbone.feat.blocks.7.attn.lora_B_v.4.weight', 'backbone.feat.blocks.11.attn.lora_B_v.4.weight', 'backbone.feat.blocks.3.attn.lora_B_k.4.weight', 'backbone.feat.blocks.6.attn.lora_B_v.4.weight', 'backbone.feat.blocks.10.attn.lora_B_v.4.weight', 'backbone.feat.blocks.8.attn.lora_B_k.4.weight', 'backbone.feat.blocks.1.attn.lora_B_v.4.weight', 'backbone.feat.blocks.2.attn.lora_B_k.4.weight', 'backbone.feat.blocks.10.attn.lora_B_k.4.weight', 'backbone.feat.blocks.7.attn.lora_B_k.4.weight', 'backbone.feat.blocks.4.attn.lora_B_v.4.weight', 'backbone.feat.blocks.5.attn.lora_B_k.4.weight', 'backbone.feat.blocks.5.attn.lora_B_v.4.weight', 'backbone.feat.blocks.0.attn.lora_B_k.4.weight', 'backbone.feat.blocks.8.attn.lora_B_v.4.weight', 'backbone.feat.blocks.6.attn.lora_B_k.4.weight', 'classifier_pool.4.bias'}
================Task 4 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.5194 	Average Acc: 84.82 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1912 	Average Acc: 94.00 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1689 	Average Acc: 94.57 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1646 	Average Acc: 94.82 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1517 	Average Acc: 95.21 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1684 	Average Acc: 94.36 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1441 	Average Acc: 95.45 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1490 	Average Acc: 95.21 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1481 	Average Acc: 95.31 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1270 	Average Acc: 95.94 
================ Test on the test set ================
 * Average Acc: 94.93 Best acc 94.93
 * Per-Task Acc:[95.79, 94.35, 95.4, 95.34, 93.79]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1416 	Average Acc: 95.47 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1461 	Average Acc: 95.18 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1387 	Average Acc: 95.37 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1383 	Average Acc: 95.74 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1362 	Average Acc: 95.53 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1629 	Average Acc: 95.20 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1456 	Average Acc: 95.47 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1305 	Average Acc: 95.68 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1352 	Average Acc: 95.51 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1473 	Average Acc: 95.33 
================ Test on the test set ================
 * Average Acc: 94.98 Best acc 94.98
 * Per-Task Acc:[95.49, 94.28, 95.14, 95.42, 94.56]
Threshold:  0.97
Skip Updating GPM for layer: 1
Skip Updating GPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768
Layer 2 : 11/768
Layer 3 : 14/768
Layer 4 : 15/768
Layer 5 : 20/768
Layer 6 : 25/768
Layer 7 : 31/768
Layer 8 : 51/768
Layer 9 : 109/768
Layer 10 : 136/768
Layer 11 : 56/768
Layer 12 : 116/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 4 Testing!================
 * Average Acc: 95.40 Best acc 95.40
 * Per-Task Acc:[96.27, 94.49, 95.71, 95.4, 95.12]
这是我个人设置的准确率记录
ACC_5:  [99.61, 97.85, 96.97, 95.83, 95.4]
平均ACC: 97.13199999999999
================Task 5 Start!================
Parameters to be updated: {'backbone.feat.blocks.11.attn.lora_B_v.5.weight', 'backbone.feat.blocks.10.attn.lora_B_k.5.weight', 'backbone.feat.blocks.8.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'backbone.feat.blocks.7.attn.lora_B_v.5.weight', 'backbone.feat.blocks.1.attn.lora_B_k.5.weight', 'backbone.feat.blocks.3.attn.lora_B_k.5.weight', 'backbone.feat.blocks.1.attn.lora_B_v.5.weight', 'backbone.feat.blocks.4.attn.lora_B_k.5.weight', 'backbone.feat.blocks.6.attn.lora_B_k.5.weight', 'backbone.feat.blocks.4.attn.lora_B_v.5.weight', 'backbone.feat.blocks.0.attn.lora_B_k.5.weight', 'backbone.feat.blocks.9.attn.lora_B_v.5.weight', 'backbone.feat.blocks.5.attn.lora_B_k.5.weight', 'backbone.feat.blocks.11.attn.lora_B_k.5.weight', 'backbone.feat.blocks.8.attn.lora_B_v.5.weight', 'backbone.feat.blocks.3.attn.lora_B_v.5.weight', 'backbone.feat.blocks.10.attn.lora_B_v.5.weight', 'backbone.feat.blocks.2.attn.lora_B_v.5.weight', 'backbone.feat.blocks.7.attn.lora_B_k.5.weight', 'backbone.feat.blocks.0.attn.lora_B_v.5.weight', 'classifier_pool.5.bias', 'backbone.feat.blocks.6.attn.lora_B_v.5.weight', 'backbone.feat.blocks.9.attn.lora_B_k.5.weight', 'backbone.feat.blocks.2.attn.lora_B_k.5.weight', 'backbone.feat.blocks.5.attn.lora_B_v.5.weight'}
================Task 5 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.4981 	Average Acc: 85.18 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1727 	Average Acc: 94.67 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1596 	Average Acc: 94.61 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1528 	Average Acc: 94.80 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1547 	Average Acc: 94.67 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1443 	Average Acc: 95.12 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1354 	Average Acc: 95.47 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1283 	Average Acc: 95.78 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1478 	Average Acc: 95.25 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1478 	Average Acc: 94.88 
================ Test on the test set ================
 * Average Acc: 93.44 Best acc 93.44
 * Per-Task Acc:[95.83, 94.03, 94.71, 95.1, 93.89, 87.09]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1353 	Average Acc: 95.57 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1170 	Average Acc: 96.23 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1311 	Average Acc: 95.18 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1274 	Average Acc: 95.88 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1163 	Average Acc: 96.35 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1213 	Average Acc: 96.15 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1065 	Average Acc: 96.48 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1263 	Average Acc: 95.78 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1132 	Average Acc: 96.37 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1198 	Average Acc: 96.31 
================ Test on the test set ================
 * Average Acc: 93.49 Best acc 93.49
 * Per-Task Acc:[95.73, 93.72, 94.61, 95.03, 93.54, 88.33]
Threshold:  0.975
Skip Updating GPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768
Layer 2 : 11/768
Layer 3 : 15/768
Layer 4 : 17/768
Layer 5 : 24/768
Layer 6 : 30/768
Layer 7 : 39/768
Layer 8 : 66/768
Layer 9 : 133/768
Layer 10 : 160/768
Layer 11 : 69/768
Layer 12 : 132/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 5 Testing!================
 * Average Acc: 94.02 Best acc 94.02
 * Per-Task Acc:[95.12, 93.34, 94.81, 95.46, 93.89, 91.5]
这是我个人设置的准确率记录
ACC_6:  [99.61, 97.85, 96.97, 95.83, 95.4, 94.02]
平均ACC: 96.61333333333333
================Task 6 Start!================
Parameters to be updated: {'backbone.feat.blocks.3.attn.lora_B_v.6.weight', 'backbone.feat.blocks.4.attn.lora_B_k.6.weight', 'backbone.feat.blocks.6.attn.lora_B_k.6.weight', 'backbone.feat.blocks.2.attn.lora_B_v.6.weight', 'backbone.feat.blocks.7.attn.lora_B_v.6.weight', 'backbone.feat.blocks.8.attn.lora_B_k.6.weight', 'backbone.feat.blocks.2.attn.lora_B_k.6.weight', 'backbone.feat.blocks.3.attn.lora_B_k.6.weight', 'backbone.feat.blocks.9.attn.lora_B_v.6.weight', 'backbone.feat.blocks.1.attn.lora_B_v.6.weight', 'backbone.feat.blocks.5.attn.lora_B_v.6.weight', 'backbone.feat.blocks.1.attn.lora_B_k.6.weight', 'backbone.feat.blocks.7.attn.lora_B_k.6.weight', 'backbone.feat.blocks.11.attn.lora_B_v.6.weight', 'backbone.feat.blocks.11.attn.lora_B_k.6.weight', 'backbone.feat.blocks.0.attn.lora_B_k.6.weight', 'backbone.feat.blocks.10.attn.lora_B_k.6.weight', 'backbone.feat.blocks.9.attn.lora_B_k.6.weight', 'backbone.feat.blocks.0.attn.lora_B_v.6.weight', 'backbone.feat.blocks.4.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'backbone.feat.blocks.5.attn.lora_B_k.6.weight', 'classifier_pool.6.weight', 'backbone.feat.blocks.6.attn.lora_B_v.6.weight', 'backbone.feat.blocks.10.attn.lora_B_v.6.weight', 'backbone.feat.blocks.8.attn.lora_B_v.6.weight'}
================Task 6 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.4622 	Average Acc: 87.01 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1435 	Average Acc: 95.70 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1330 	Average Acc: 96.00 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1307 	Average Acc: 95.41 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1217 	Average Acc: 95.90 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1577 	Average Acc: 95.08 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1385 	Average Acc: 95.31 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1348 	Average Acc: 95.84 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1280 	Average Acc: 96.04 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1204 	Average Acc: 96.15 
================ Test on the test set ================
 * Average Acc: 93.64 Best acc 93.64
 * Per-Task Acc:[94.81, 93.05, 94.69, 94.39, 92.79, 90.06, 95.71]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1179 	Average Acc: 96.37 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1217 	Average Acc: 96.23 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1059 	Average Acc: 96.50 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1063 	Average Acc: 96.60 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1004 	Average Acc: 96.88 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.0969 	Average Acc: 96.89 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1116 	Average Acc: 96.15 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1349 	Average Acc: 95.72 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1108 	Average Acc: 96.64 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.0932 	Average Acc: 97.27 
================ Test on the test set ================
 * Average Acc: 93.65 Best acc 93.65
 * Per-Task Acc:[94.42, 93.03, 94.37, 93.67, 93.17, 90.39, 96.51]
Threshold:  0.98
Skip Updating GPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768
Layer 2 : 12/768
Layer 3 : 16/768
Layer 4 : 20/768
Layer 5 : 27/768
Layer 6 : 34/768
Layer 7 : 49/768
Layer 8 : 83/768
Layer 9 : 164/768
Layer 10 : 198/768
Layer 11 : 100/768
Layer 12 : 173/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 6 Testing!================
 * Average Acc: 93.91 Best acc 93.91
 * Per-Task Acc:[94.37, 93.07, 94.43, 94.25, 93.64, 91.83, 95.76]
这是我个人设置的准确率记录
ACC_7:  [99.61, 97.85, 96.97, 95.83, 95.4, 94.02, 93.91]
平均ACC: 96.22714285714285
================Task 7 Start!================
Parameters to be updated: {'backbone.feat.blocks.1.attn.lora_B_k.7.weight', 'backbone.feat.blocks.7.attn.lora_B_v.7.weight', 'backbone.feat.blocks.1.attn.lora_B_v.7.weight', 'backbone.feat.blocks.7.attn.lora_B_k.7.weight', 'backbone.feat.blocks.0.attn.lora_B_v.7.weight', 'backbone.feat.blocks.10.attn.lora_B_v.7.weight', 'backbone.feat.blocks.4.attn.lora_B_k.7.weight', 'backbone.feat.blocks.3.attn.lora_B_v.7.weight', 'backbone.feat.blocks.9.attn.lora_B_k.7.weight', 'backbone.feat.blocks.5.attn.lora_B_k.7.weight', 'classifier_pool.7.weight', 'backbone.feat.blocks.4.attn.lora_B_v.7.weight', 'backbone.feat.blocks.6.attn.lora_B_v.7.weight', 'backbone.feat.blocks.0.attn.lora_B_k.7.weight', 'backbone.feat.blocks.9.attn.lora_B_v.7.weight', 'backbone.feat.blocks.2.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'backbone.feat.blocks.10.attn.lora_B_k.7.weight', 'backbone.feat.blocks.6.attn.lora_B_k.7.weight', 'backbone.feat.blocks.11.attn.lora_B_k.7.weight', 'backbone.feat.blocks.8.attn.lora_B_k.7.weight', 'backbone.feat.blocks.3.attn.lora_B_k.7.weight', 'backbone.feat.blocks.5.attn.lora_B_v.7.weight', 'backbone.feat.blocks.2.attn.lora_B_v.7.weight', 'backbone.feat.blocks.11.attn.lora_B_v.7.weight', 'backbone.feat.blocks.8.attn.lora_B_v.7.weight'}
================Task 7 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.5967 	Average Acc: 81.84 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.2908 	Average Acc: 90.33 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.2461 	Average Acc: 92.03 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.2445 	Average Acc: 92.46 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.2232 	Average Acc: 92.48 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.2052 	Average Acc: 93.05 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.2171 	Average Acc: 92.60 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.2236 	Average Acc: 92.71 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.2031 	Average Acc: 93.46 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.2150 	Average Acc: 92.95 
================ Test on the test set ================
 * Average Acc: 91.68 Best acc 91.68
 * Per-Task Acc:[93.87, 92.74, 93.93, 94.44, 92.41, 88.5, 95.42, 82.15]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.2229 	Average Acc: 92.95 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.2104 	Average Acc: 92.62 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1925 	Average Acc: 93.85 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1925 	Average Acc: 93.65 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1909 	Average Acc: 93.50 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1986 	Average Acc: 93.07 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1865 	Average Acc: 93.28 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1909 	Average Acc: 93.79 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1891 	Average Acc: 93.77 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.2013 	Average Acc: 93.14 
================ Test on the test set ================
 * Average Acc: 91.81 Best acc 91.81
 * Per-Task Acc:[93.94, 92.69, 93.86, 94.3, 91.94, 87.71, 95.4, 84.62]
Threshold:  0.985
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768
Layer 2 : 13/768
Layer 3 : 18/768
Layer 4 : 24/768
Layer 5 : 33/768
Layer 6 : 45/768
Layer 7 : 62/768
Layer 8 : 102/768
Layer 9 : 194/768
Layer 10 : 246/768
Layer 11 : 137/768
Layer 12 : 208/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 7 Testing!================
 * Average Acc: 92.50 Best acc 92.50
 * Per-Task Acc:[93.91, 92.47, 93.21, 94.47, 92.44, 89.92, 95.57, 88.06]
这是我个人设置的准确率记录
ACC_8:  [99.61, 97.85, 96.97, 95.83, 95.4, 94.02, 93.91, 92.5]
平均ACC: 95.76125
================Task 8 Start!================
Parameters to be updated: {'backbone.feat.blocks.7.attn.lora_B_v.8.weight', 'backbone.feat.blocks.2.attn.lora_B_v.8.weight', 'backbone.feat.blocks.3.attn.lora_B_k.8.weight', 'backbone.feat.blocks.9.attn.lora_B_k.8.weight', 'backbone.feat.blocks.5.attn.lora_B_v.8.weight', 'backbone.feat.blocks.2.attn.lora_B_k.8.weight', 'backbone.feat.blocks.11.attn.lora_B_v.8.weight', 'backbone.feat.blocks.11.attn.lora_B_k.8.weight', 'backbone.feat.blocks.1.attn.lora_B_k.8.weight', 'backbone.feat.blocks.4.attn.lora_B_v.8.weight', 'backbone.feat.blocks.6.attn.lora_B_k.8.weight', 'backbone.feat.blocks.4.attn.lora_B_k.8.weight', 'backbone.feat.blocks.7.attn.lora_B_k.8.weight', 'backbone.feat.blocks.10.attn.lora_B_v.8.weight', 'backbone.feat.blocks.9.attn.lora_B_v.8.weight', 'backbone.feat.blocks.3.attn.lora_B_v.8.weight', 'backbone.feat.blocks.0.attn.lora_B_v.8.weight', 'backbone.feat.blocks.1.attn.lora_B_v.8.weight', 'backbone.feat.blocks.5.attn.lora_B_k.8.weight', 'backbone.feat.blocks.10.attn.lora_B_k.8.weight', 'backbone.feat.blocks.8.attn.lora_B_k.8.weight', 'backbone.feat.blocks.0.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'backbone.feat.blocks.8.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'backbone.feat.blocks.6.attn.lora_B_v.8.weight'}
================Task 8 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.5033 	Average Acc: 85.61 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.1725 	Average Acc: 94.38 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.1610 	Average Acc: 94.86 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.1492 	Average Acc: 95.00 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.1642 	Average Acc: 95.06 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.1557 	Average Acc: 95.18 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.1584 	Average Acc: 95.12 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.1509 	Average Acc: 95.12 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.1479 	Average Acc: 95.04 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.1813 	Average Acc: 94.45 
================ Test on the test set ================
 * Average Acc: 91.50 Best acc 91.50
 * Per-Task Acc:[93.6, 92.43, 92.35, 94.25, 92.17, 87.89, 93.74, 86.89, 90.21]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1573 	Average Acc: 95.37 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1492 	Average Acc: 95.27 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1418 	Average Acc: 95.80 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1425 	Average Acc: 95.20 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1484 	Average Acc: 95.06 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1375 	Average Acc: 95.72 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1257 	Average Acc: 95.80 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1265 	Average Acc: 96.00 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1373 	Average Acc: 95.86 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1319 	Average Acc: 95.51 
================ Test on the test set ================
 * Average Acc: 91.51 Best acc 91.51
 * Per-Task Acc:[93.58, 92.33, 92.5, 94.15, 91.81, 87.21, 93.92, 85.82, 92.26]
Threshold:  0.99
Skip Updating GPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768
Layer 2 : 16/768
Layer 3 : 22/768
Layer 4 : 31/768
Layer 5 : 41/768
Layer 6 : 63/768
Layer 7 : 90/768
Layer 8 : 149/768
Layer 9 : 266/768
Layer 10 : 330/768
Layer 11 : 221/768
Layer 12 : 306/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 8 Testing!================
 * Average Acc: 91.89 Best acc 91.89
 * Per-Task Acc:[92.92, 93.13, 92.72, 93.64, 92.23, 88.59, 94.52, 87.85, 91.42]
这是我个人设置的准确率记录
ACC_9:  [99.61, 97.85, 96.97, 95.83, 95.4, 94.02, 93.91, 92.5, 91.89]
平均ACC: 95.33111111111111
================Task 9 Start!================
Parameters to be updated: {'backbone.feat.blocks.10.attn.lora_B_v.9.weight', 'backbone.feat.blocks.0.attn.lora_B_k.9.weight', 'backbone.feat.blocks.11.attn.lora_B_k.9.weight', 'backbone.feat.blocks.8.attn.lora_B_k.9.weight', 'backbone.feat.blocks.3.attn.lora_B_v.9.weight', 'backbone.feat.blocks.5.attn.lora_B_v.9.weight', 'backbone.feat.blocks.6.attn.lora_B_v.9.weight', 'backbone.feat.blocks.7.attn.lora_B_k.9.weight', 'backbone.feat.blocks.11.attn.lora_B_v.9.weight', 'backbone.feat.blocks.1.attn.lora_B_k.9.weight', 'backbone.feat.blocks.7.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'backbone.feat.blocks.3.attn.lora_B_k.9.weight', 'backbone.feat.blocks.8.attn.lora_B_v.9.weight', 'backbone.feat.blocks.5.attn.lora_B_k.9.weight', 'backbone.feat.blocks.10.attn.lora_B_k.9.weight', 'backbone.feat.blocks.6.attn.lora_B_k.9.weight', 'backbone.feat.blocks.4.attn.lora_B_v.9.weight', 'backbone.feat.blocks.9.attn.lora_B_v.9.weight', 'backbone.feat.blocks.2.attn.lora_B_v.9.weight', 'backbone.feat.blocks.2.attn.lora_B_k.9.weight', 'backbone.feat.blocks.4.attn.lora_B_k.9.weight', 'backbone.feat.blocks.1.attn.lora_B_v.9.weight', 'backbone.feat.blocks.9.attn.lora_B_k.9.weight', 'backbone.feat.blocks.0.attn.lora_B_v.9.weight', 'classifier_pool.9.weight'}
================Task 9 Training!================
The training samples number: 5000
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [0/20] |	Loss: 0.6328 	Average Acc: 79.51 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [1/20] |	Loss: 0.2480 	Average Acc: 91.11 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [2/20] |	Loss: 0.2286 	Average Acc: 92.11 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [3/20] |	Loss: 0.2242 	Average Acc: 92.05 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [4/20] |	Loss: 0.2064 	Average Acc: 93.12 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [5/20] |	Loss: 0.2071 	Average Acc: 92.56 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [6/20] |	Loss: 0.2057 	Average Acc: 93.03 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [7/20] |	Loss: 0.2148 	Average Acc: 92.52 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [8/20] |	Loss: 0.2178 	Average Acc: 92.73 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [9/20] |	Loss: 0.2189 	Average Acc: 92.75 
================ Test on the test set ================
 * Average Acc: 90.79 Best acc 90.79
 * Per-Task Acc:[91.08, 93.03, 92.25, 93.06, 91.66, 88.74, 94.37, 87.75, 91.14, 84.8]
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [10/20] |	Loss: 0.1847 	Average Acc: 93.83 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [11/20] |	Loss: 0.1978 	Average Acc: 93.44 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [12/20] |	Loss: 0.1780 	Average Acc: 93.95 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [13/20] |	Loss: 0.1732 	Average Acc: 93.73 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [14/20] |	Loss: 0.1824 	Average Acc: 93.85 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [15/20] |	Loss: 0.1630 	Average Acc: 94.36 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [16/20] |	Loss: 0.1661 	Average Acc: 94.32 
learning rate: [0.001, 0.01]
================ Train on the train set ================
Epoch [17/20] |	Loss: 0.1869 	Average Acc: 93.52 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [18/20] |	Loss: 0.1982 	Average Acc: 92.93 
learning rate: [0.0001, 0.001]
================ Train on the train set ================
Epoch [19/20] |	Loss: 0.1625 	Average Acc: 94.38 
================ Test on the test set ================
 * Average Acc: 90.73 Best acc 90.79
 * Per-Task Acc:[91.25, 92.83, 92.26, 92.89, 91.45, 88.38, 94.25, 88.18, 90.86, 84.92]
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768
Layer 2 : 18/768
Layer 3 : 29/768
Layer 4 : 44/768
Layer 5 : 57/768
Layer 6 : 97/768
Layer 7 : 147/768
Layer 8 : 235/768
Layer 9 : 371/768
Layer 10 : 445/768
Layer 11 : 334/768
Layer 12 : 432/768
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
================ InfLoRA_CA1 (stage2)================
================Task 9 Testing!================
 * Average Acc: 91.21 Best acc 91.21
 * Per-Task Acc:[89.99, 92.23, 92.03, 92.94, 91.77, 88.93, 94.64, 87.33, 91.63, 90.59]
这是我个人设置的准确率记录
ACC_10:  [99.61, 97.85, 96.97, 95.83, 95.4, 94.02, 93.91, 92.5, 91.89, 91.21]
平均ACC: 94.91900000000001
Time cost :  15673.38558769226
